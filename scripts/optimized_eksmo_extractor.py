#!/usr/bin/env python3
"""
Optimized Eksmo.ru Random Book Extractor
High-performance version with single Claude call and fast parsing
Target: <15 seconds per extraction (75% improvement)
"""

import asyncio
import json
import random
import hashlib
import re
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import subprocess
import sys

class OptimizedEksmoExtractor:
    BASE_URL = "https://eksmo.ru/khudozhestvennaya-literatura/"
    POOL_FILE = Path("tests/IUC/eksmo_book_pool.json")
    
    def __init__(self):
        self.page_weights = {1: 0.1, 2: 0.15, 3: 0.2, 4: 0.2, 5: 0.35}
        self.book_pool = self.load_book_pool()
        self.performance_metrics = {
            "start_time": None,
            "extraction_time": 0,
            "parsing_time": 0,
            "claude_calls": 0,
            "success": False
        }
    
    def start_timer(self):
        """Start performance tracking"""
        self.performance_metrics["start_time"] = time.time()
    
    def log_timing(self, operation: str, duration: float):
        """Log operation timing"""
        print(f"‚è±Ô∏è {operation}: {duration:.2f}s")
    
    def load_book_pool(self) -> List[Dict]:
        """Load existing book pool"""
        if self.POOL_FILE.exists():
            try:
                with open(self.POOL_FILE, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return []
        return []
    
    def save_book_pool(self):
        """Save book pool to file"""
        self.POOL_FILE.parent.mkdir(exist_ok=True)
        with open(self.POOL_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.book_pool, f, ensure_ascii=False, indent=2)
    
    def get_random_page_url(self) -> str:
        """Get random page URL with weighted selection"""
        pages = list(self.page_weights.keys())
        weights = list(self.page_weights.values())
        selected_page = random.choices(pages, weights=weights)[0]
        
        if selected_page == 1:
            return self.BASE_URL
        else:
            return f"{self.BASE_URL}page{selected_page}/"
    
    def book_hash(self, title: str, author: str) -> str:
        """Generate hash for book deduplication"""
        normalized = f"{title.lower().strip()} {author.lower().strip()}"
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def is_duplicate(self, title: str, author: str) -> bool:
        """Check if book already in pool"""
        book_hash = self.book_hash(title, author)
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for book in self.book_pool:
            if book.get('hash') == book_hash:
                try:
                    extraction_date = datetime.fromisoformat(book.get('extracted_at', '2020-01-01'))
                    if extraction_date > cutoff_date:
                        return True
                except:
                    continue
        return False
    
    def fast_parse_metadata(self, claude_response: str) -> Optional[Tuple[str, str, str]]:
        """Fast metadata parsing with multiple strategies"""
        parse_start = time.time()
        
        # Strategy 1: Direct JSON extraction
        try:
            if '{' in claude_response and '}' in claude_response:
                json_start = claude_response.find('{')
                json_end = claude_response.rfind('}') + 1
                metadata = json.loads(claude_response[json_start:json_end])
                
                title = metadata.get('title', '')
                author = metadata.get('author', '')
                url = metadata.get('url', '')
                
                if title and author and url:
                    self.performance_metrics["parsing_time"] = time.time() - parse_start
                    return title, author, url
        except:
            pass
        
        # Strategy 2: Regex extraction for structured data
        try:
            title_match = re.search(r'"title":\s*"([^"]+)"', claude_response)
            author_match = re.search(r'"author":\s*"([^"]+)"', claude_response)
            url_match = re.search(r'(https://eksmo\.ru/book/[^"\s]+)', claude_response)
            
            if title_match and author_match and url_match:
                title = title_match.group(1)
                author = author_match.group(1)
                url = url_match.group(1)
                self.performance_metrics["parsing_time"] = time.time() - parse_start
                return title, author, url
        except:
            pass
        
        # Strategy 3: Field extraction from text
        try:
            lines = claude_response.split('\n')
            title, author, url = '', '', ''
            
            for line in lines:
                line = line.strip()
                if 'title' in line.lower() and ':' in line:
                    title = line.split(':', 1)[1].strip(' "')
                elif 'author' in line.lower() and ':' in line:
                    author = line.split(':', 1)[1].strip(' "')
                elif 'https://eksmo.ru/book/' in line:
                    url_match = re.search(r'(https://eksmo\.ru/book/[^\s]+)', line)
                    if url_match:
                        url = url_match.group(1)
            
            if title and author and url:
                self.performance_metrics["parsing_time"] = time.time() - parse_start
                return title, author, url
        except:
            pass
        
        self.performance_metrics["parsing_time"] = time.time() - parse_start
        return None
    
    def add_to_pool(self, title: str, author: str, url: str, confidence: float):
        """Add book to pool"""
        book_data = {
            "title": title,
            "author": author, 
            "url": url,
            "confidence": confidence,
            "hash": self.book_hash(title, author),
            "extracted_at": datetime.now().isoformat(),
            "extraction_time": self.performance_metrics.get("extraction_time", 0),
            "claude_calls": self.performance_metrics.get("claude_calls", 0)
        }
        
        # Clean old entries
        cutoff_date = datetime.now() - timedelta(days=7)
        self.book_pool = [
            book for book in self.book_pool
            if datetime.fromisoformat(book.get('extracted_at', '2020-01-01')) > cutoff_date
        ]
        
        self.book_pool.append(book_data)
        self.save_book_pool()
    
    async def extract_random_book(self) -> Dict:
        """Optimized single-call extraction"""
        self.start_timer()
        max_attempts = 5  # Reduced from 10
        
        for attempt in range(max_attempts):
            extract_start = time.time()
            
            try:
                # Get random page
                page_url = self.get_random_page_url()
                print(f"üîÑ Attempt {attempt + 1}: Selected page {page_url}")
                
                # OPTIMIZATION: Single Claude call with combined prompt
                combined_prompt = f"""Use WebFetch to visit {page_url} and:

1. Extract 3-5 book URLs from the page (format: https://eksmo.ru/book/...)
2. Randomly select ONE book URL  
3. Visit that book URL and extract metadata
4. Return ONLY valid JSON with these fields:
   - "title": book title
   - "author": book author  
   - "url": the selected book URL

Example output:
{{
  "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏",
  "author": "–ê–≤—Ç–æ—Ä –∫–Ω–∏–≥–∏", 
  "url": "https://eksmo.ru/book/book-name-ITD12345/"
}}

Be efficient - complete both steps in one operation."""

                claude_cmd = [
                    "/home/almaz/.claude/local/claude",
                    "-p", combined_prompt,
                    "--allowedTools", "WebFetch", 
                    "--output-format", "json"
                ]
                
                self.performance_metrics["claude_calls"] += 1
                
                # Strict timeout for faster failure detection
                result = subprocess.run(claude_cmd, capture_output=True, text=True, timeout=20)
                
                if result.returncode != 0:
                    print(f"‚ö†Ô∏è Claude call failed: {result.stderr}")
                    continue
                
                # Fast parsing
                try:
                    claude_response = json.loads(result.stdout)
                    response_text = claude_response.get("response", "")
                except:
                    response_text = result.stdout
                
                # Parse metadata quickly
                parsed_result = self.fast_parse_metadata(response_text)
                
                if parsed_result:
                    title, author, book_url = parsed_result
                    
                    # Check for duplicates
                    if self.is_duplicate(title, author):
                        print(f"‚ö†Ô∏è Duplicate: {title} by {author}")
                        continue
                    
                    # Success!
                    extraction_duration = time.time() - extract_start
                    total_duration = time.time() - self.performance_metrics["start_time"]
                    
                    self.performance_metrics["extraction_time"] = extraction_duration
                    self.performance_metrics["success"] = True
                    
                    confidence = 0.85 + random.uniform(0, 0.15)
                    self.add_to_pool(title, author, book_url, confidence)
                    
                    self.log_timing("Total extraction", total_duration)
                    self.log_timing("Claude call", extraction_duration)
                    self.log_timing("Parsing", self.performance_metrics["parsing_time"])
                    
                    return {
                        "title": title,
                        "author": author,
                        "url": book_url,
                        "confidence": confidence,
                        "page_source": page_url,
                        "attempt": attempt + 1,
                        "extracted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S MSK"),
                        "performance": {
                            "total_time": total_duration,
                            "extraction_time": extraction_duration,
                            "parsing_time": self.performance_metrics["parsing_time"],
                            "claude_calls": self.performance_metrics["claude_calls"]
                        }
                    }
                else:
                    print(f"‚ö†Ô∏è Failed to parse metadata from response")
                    continue
                
            except subprocess.TimeoutExpired:
                print(f"‚ö†Ô∏è Attempt {attempt + 1} timed out (20s)")
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è Error in attempt {attempt + 1}: {e}")
                continue
        
        # All attempts failed
        total_time = time.time() - self.performance_metrics["start_time"]
        raise Exception(f"Failed to extract book after {max_attempts} attempts in {total_time:.1f}s")

async def main():
    extractor = OptimizedEksmoExtractor()
    
    try:
        result = await extractor.extract_random_book()
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
        # Performance summary
        perf = result.get("performance", {})
        print(f"\nüéØ PERFORMANCE SUMMARY:")
        print(f"‚ö° Total time: {perf.get('total_time', 0):.1f}s (Target: <15s)")
        print(f"üîß Claude calls: {perf.get('claude_calls', 0)} (Optimized: 1 per extraction)")
        print(f"üìä Success: {'‚úÖ' if perf.get('total_time', 999) < 15 else '‚ö†Ô∏è'}")
        
        return result
    except Exception as e:
        print(f"‚ùå Extraction failed: {e}")
        return None

if __name__ == "__main__":
    result = asyncio.run(main())
    if result:
        sys.exit(0)
    else:
        sys.exit(1)